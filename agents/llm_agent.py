#!/usr/bin/env python3
"""
LLM Agent - Handles recipe processing via chat interface

This agent allows users to interact via chat to trigger recipe processing workflows.
It can receive recipe text via chat and trigger the recipe processing pipeline.
"""

import asyncio
import sys
from pathlib import Path
import json
import logging

# Add src to path for development
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / "src"))

from openagents.agents.collaborator_agent import CollaboratorAgent
from openagents.models.event_context import EventContext
from openagents.models.agent_config import AgentConfig

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LlmAgent(CollaboratorAgent):
    """An LLM agent that handles recipe processing via chat interface."""

    default_agent_id = "llm-agent"

    def __init__(self, **kwargs):
        # Create agent config
        agent_config = AgentConfig(
            instruction="""æ‚¨æ˜¯ä¸€ä¸ªèœè°±å¤„ç†åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·å¤„ç†èœè°±ç›¸å…³çš„è¯·æ±‚ã€‚

            æ‚¨å¯ä»¥ï¼š
            1. æ¥æ”¶ç”¨æˆ·è¾“å…¥çš„èœè°±æ–‡æœ¬
            2. å°†èœè°±æ–‡æœ¬æ ‡å‡†åŒ–ä¸ºmarkdownæ ¼å¼
            3. è§¦å‘èœè°±å¡ç‰‡ç”Ÿæˆæµç¨‹

            å½“ç”¨æˆ·æäº¤èœè°±å†…å®¹æ—¶ï¼Œæ‚¨åº”è¯¥ï¼š
            - ç†è§£ç”¨æˆ·æäº¤çš„èœè°±å†…å®¹
            - å°†èœè°±å†…å®¹è½¬æ¢ä¸ºæ ‡å‡†markdownæ ¼å¼
            - è§¦å‘åç»­å¤„ç†æµç¨‹ç”Ÿæˆèœè°±å¡ç‰‡""",
            react_to_all_messages=True
        )
        super().__init__(agent_config=agent_config, **kwargs)

    async def on_startup(self):
        """Called when agent starts and connects to the network."""
        logger.info("LLM Agent is running! Press Ctrl+C to stop.")
        logger.info("Ready to process recipe requests via chat...")

    async def on_shutdown(self):
        """Called when agent shuts down."""
        logger.info("LLM Agent stopped.")

    async def react(self, context: EventContext):
        """React to incoming messages by processing recipe requests."""
        # Skip our own messages
        if context.incoming_event.source_id == self.agent_id:
            return

        logger.info(f"Event received: {context.incoming_event.event_name}")
        logger.info(f"Event source: {context.incoming_event.source_id}")
        
        # Process chat messages that contain recipe requests
        payload = context.incoming_event.payload
        content = ""
        
        # Check if payload is a dict and has content
        if isinstance(payload, dict):
            # Try different possible keys for content
            content = payload.get("content", "")
            if not content:
                content = payload.get("text", "")
            if not content and "message" in payload:
                content = payload["message"]
            # If content is still empty, try to convert the entire payload to string
            if not content:
                content = str(payload)
        else:
            # If payload is not a dict, it might already be the content
            content = str(payload) if payload else ""
        
        # Ensure content is a string
        if not isinstance(content, str):
            content = str(content)
        
        if content:
            # Check if this looks like a recipe request
            if any(keyword in content.lower() for keyword in 
                  ["èœè°±", "recipe", "åšæ³•", "cooking", "cook", "how to make", "æ€ä¹ˆåš", "é£Ÿè°±", "ingredients", "instructions"]):
                logger.info(f"Processing recipe request: {content[:100]}...")
                
                # Trigger the recipe processing workflow
                await self._trigger_recipe_processing(content, context)
            else:
                # For other messages, just acknowledge
                response = f"æ”¶åˆ°æ‚¨çš„æ¶ˆæ¯ã€‚å¦‚æœæ‚¨æƒ³åˆ›å»ºèœè°±å¡ç‰‡ï¼Œè¯·æä¾›èœè°±å†…å®¹ï¼Œæˆ‘ä¼šå¸®æ‚¨å¤„ç†ã€‚"
                await self.send_direct_message(context.incoming_event.source_id, response)

    async def _trigger_recipe_processing(self, recipe_content, context):
        """Trigger the recipe processing workflow."""
        try:
            logger.info("å¼€å§‹è§¦å‘èœè°±å¤„ç†æµç¨‹...")
            
            # Send a message to the user acknowledging the request
            await self.send_direct_message(
                context.incoming_event.source_id, 
                "æ­£åœ¨å¤„ç†æ‚¨çš„èœè°±è¯·æ±‚ï¼Œå°†å…¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼..."
            )
            
            # Publish the recipe text event to trigger the processing pipeline
            await context.create_event(
                name="recipe.text.transcribed",  # This matches the event polisher listens for
                payload={
                    "content": recipe_content,
                    "recipe_id": f"recipe_{context.incoming_event.id}" if hasattr(context.incoming_event, 'id') else "manual_recipe",
                    "source": "llm_agent"
                }
            )
            
            logger.info("èœè°±å¤„ç†äº‹ä»¶å·²å‘å¸ƒï¼Œç­‰å¾…åç»­å¤„ç†...")
            
            # Send a message to the recipe processing channel
            messaging = self.client.mod_adapters.get("openagents.mods.workspace.messaging")
            if messaging:
                await messaging.send_channel_message(
                    channel="recipe.md",
                    text=f"ğŸ“ æ–°èœè°±å·²æäº¤ï¼Œæ­£åœ¨å¤„ç†ä¸­..."
                )
                
        except Exception as e:
            logger.error(f"å¤„ç†èœè°±è¯·æ±‚æ—¶å‡ºé”™: {e}")
            await self.send_direct_message(
                context.incoming_event.source_id,
                f"å¤„ç†èœè°±æ—¶å‡ºç°é”™è¯¯: {str(e)}"
            )


async def main():
    """Run the LLM Agent."""
    import argparse

    parser = argparse.ArgumentParser(description="LLM Agent for Recipe Processing")
    parser.add_argument("--host", default="localhost", help="Network host")
    parser.add_argument("--port", type=int, default=8700, help="Network port")
    parser.add_argument("--url", default=None, help="Connection URL (e.g., grpc://localhost:8600 for direct gRPC)")
    args = parser.parse_args()

    agent = LlmAgent()

    try:
        if args.url:
            # Use URL for direct connection (useful for Docker port mapping)
            print(f"å°è¯•è¿æ¥åˆ° {args.url}...")
            await agent.async_start(url=args.url)
        else:
            print(f"å°è¯•è¿æ¥åˆ°ç½‘ç»œ {args.host}:{args.port}...")
            await agent.async_start(
                network_host=args.host,
                network_port=args.port,
            )

        print("LLM Agent å¯åŠ¨æˆåŠŸï¼Œç­‰å¾…æ¶ˆæ¯...")
        
        # Keep running until interrupted
        while True:
            await asyncio.sleep(1)

    except KeyboardInterrupt:
        print("\nShutting down...")
    except Exception as e:
        print(f"å¯åŠ¨ Agent æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("åœæ­¢ Agent...")
        try:
            await agent.async_stop()
        except:
            pass  # å¦‚æœ agent æ²¡æœ‰æˆåŠŸå¯åŠ¨ï¼Œåœæ­¢æ—¶å¯èƒ½ä¼šå‡ºé”™


if __name__ == "__main__":
    asyncio.run(main())